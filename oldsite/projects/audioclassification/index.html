<!DOCTYPE html>

<html>

<head>
  <title>Audio Classification</title>
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#2b5797">
  <meta name="theme-color" content="#ffffff">
  <link rel="stylesheet" href="../../taylorStyle.css" type="text/css">
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body class="light-mode">
  <div id="mySidebar" class="sidebar">
    <a href="#Summary">Summary</a>
    <a href="#Background">Background</a>
    <a href="#Data">Data</a>
    <a href="#Preprocessing">Preprocessing</a>
    <a href="#Model">Model</a>
  </div>

  <div class="main">
    
    <h2 id="Summary">Summary</h2>
    <p>
      <ul>
        <li>Deep learning classification of Japanese and English audio</li>
        <li>Audio conversion to mel spectrograms</li>
        <li>Convolutional neural network on mel spectrograms</li>
        <li>Hyperparameter tuning for optimization</li>
        <li>Classification accuracy of 95%</li>
      </ul>
    </p>
    <h2 id="Background">Background</h2>
    <p>
      The goal of this project is to use deep learning and classify audio into two different languages, English and Japanese.
      Language classification, and audio classification in general, is a topic that is used is many applications, including IoT devices, translators, and more.
      As the world becomes more connected the need for natural langugage processing (NLP) is becoming more and more necessary.
      This project will go over through some of the fundamental principals of audio classification when building a model.
    </p>

    <h2 id="Data">Data</h2>
    <h3>Collection</h3>
    <p>
      For this project a large set of labeled audio is needed to train.
      Originally a series of audio books was used to train the model.
      This was supplemented by open source audio books from <a href="http://aozoraroudoku.jp/">aozora</a> and <a href="https://librivox.org">librivox</a>.
      One issue when training from audiobooks is that all samples of audio from the book have similar features. 
      For example, if all Japanese audio came from one book and all English from another, then a machine learning model would not have to learn the languages but instead would only have to learn the voice of the speaker or the tone. 
      This would hinder generalization to other audio sources.
      The audio source was then changed to the <a href="https://commonvoice.mozilla.org/">Common Voice</a> corpus which provides a large variety of audio clips from people in different languages.
      At the time of testing, the English audio consisted on 2200 hours of audio, and the Japanese 46 hours. 
    </p>

    <h3>Data Cleaning</h3>
    <p>
      The raw audio needs to be standardized. Some of the audiobooks are at different volumes, rates, lengths, and channels.
      Longer audio has to be cut down and shorter audio needs to be padded with silence.
      To minimize both of these, the distribution of audio length of the first 1000 samples was looked at.
      <img class='plot' src="images/length_distribution.png">
      I chose 4.0 seconds as the clip length to convert audio to.
      This is nearly in the center of both languages.
      To better augment the data, the padding was randomly split between the front and the end of the audio, and the cuts were taken at random locations for each sample. 
      For each clip, I then performed the following to further standardize:
      <ul>
        <li>Resampling to 22050 hz using  <a href="https://www.tensorflow.org/io/api_docs/python/tfio/audio/resample">tfio.audio.resample</a></li>
        <li>Normalizing the sample values to fall between 0 and 1</li>
        <li>Combine all channels into one channel</li>
      </ul>
    </p>

    <h2 id="Preprocessing">Preprocessing</h2>
    <p>
      Because of the length and the quantity of the audio files, it would not be feasible to load all of the samples at once for training.
      Therefore, data generators were built to load one batch of audio at a time. 
      A generator class was subclassed from the keras.utils.Sequence class in tensorflow. 
      This class takes in a pandas dataframe with the file paths and outputs batches of mel spectrograms for the model to use.
      
      The generator loads a preprocessing class that performs the cleaning as stated in the previous section.
      It then converts the audio to a spectrogram using <a href='https://en.wikipedia.org/wiki/Short-time_Fourier_transform'>Short-time Fourier transform</a>.
      The number of fft points, window size, and hop length need to be chosen carefully.
      If chosen poorly, the output spectrogram will be too small and will not be able to be run through the model.
      It may also result in audio sections being counted more frequently than others or some not being counted at all.
      The values were varied slighty when treated as hyperparameters later.
      This resulting spectrogram was then converted to a log-mel-spectrogram, a system more useful for differentiating human speech.
      The resulting spectrogram is then used as the input to the model. 
    </p>

    <h2 id="Model">Model</h2>
    <p>
      A deep learning convolutional neural network (CNN) was used as the model. 
      The input is a 2D mel-spectrogram and the output is a binary classifier,
      The model consists of 6 convolutional layers and 6 pooling layers. 
      The ouput of the CNN head was flattened and fed into a dense layer before being fed into the output layer.
      <ul>
        <li>Conv2D + MaxPooling2D, 16 filters, 3\(\times\)3 kernel</li>
        <li>Conv2D + MaxPooling2D, 32 filters, 3\(\times\)3 kernel</li>
        <li>Conv2D + MaxPooling2D, 64 filters, 3\(\times\)3 kernel</li>
        <li>Conv2D + MaxPooling2D, 128 filters, 3\(\times\)3 kernel</li>
        <li>Conv2D + MaxPooling2D, 265 filters, 3\(\times\)3 kernel</li>
        <li>Conv2D + MaxPooling2D, 512 filters, 3\(\times\)3 kernel</li>
        <li>Flattening </li>
        <li>Dense layer, 1024 nodes </li>
        <li>Dropout layer, 20%</li>
        <li>Output layer, 1 node</li>
      </ul>
      <img class="plot" src="images/model.png">
    </p>
    <h2 id="Results">Results</h2>
    <p>
      printResults()
    </p>
    <br>

  </div>
</body>

</html>
