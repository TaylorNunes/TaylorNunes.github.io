<!DOCTYPE html>

<html>

<head>
  <title>Language Classification</title>
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#2b5797">
  <meta name="theme-color" content="#ffffff">
  <link rel="stylesheet" href="../../taylorStyle.css" type="text/css">
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body class="light-mode">
  <div id="mySidebar" class="sidebar">
    <a href="#Summary">Summary</a>
    <a href="#Background">Background</a>
    <a href="#Data">Data</a>
    <a href="#Preprocessing">Preprocessing</a>
    <a href="#Model">Model</a>
  </div>

  <div class="main">
    
    <h2 id="Summary">Summary</h2>
    <p>
      <ul>
        <li>Collected 200,000 English and Japanese sentences</li>
        <li>Performed principal component analysis based on letter frequency</li>
        <li>Trained a logistic regression model</li>
        <li>Obtained a classification accuracy of <strong>97%</strong></li>
      </ul>
      <img class="plot" src="images/decision_logreg_test.png"></img>
    </p>
    <h2 id="Background">Background</h2>
    <p>
      As someone who has been learning Japanese for several years, I have developed an interest in linguistics and the differences between the two langugages.
      Japanese uses a character system that has three different scripts. These are hiragana, katakana, and kanji. 
      English of course doesnt use any of these scipts but uses latin characters instead. 
      If one were to classify English and Japanese sentences, it would be rather easy solely based on the characters, but it would also be less interesting. 
      To better exemplify the differences in the languages, I wanted to convert both to the same character system.
      Luckily there is a standard format of converting Japanese to roman characters (romaji) called hepburn. 
      This allows a rather direct mapping of Japanese sounds.
      This is because, both hiragana and katakana are phonetic, allowing direct conversion, and kanji can be converted to hiragana.
      Of course the other option would be to convert English words into hiragana or katakana, but due to the larger varity of phonemes in English, there would be a significant amount of loss.
      Therefore this project compares and classifies English and Japanese sentences where the Japanese sentences are written in romaji.
    </p>

    <h2 id="Data">Data</h2>
    <h3>Collection</h3>
    <p>
      To classify each language we need a large selection of Japanese and English sentences. 
      One good source of example sentences well known among language learners is the website <a href="https://tatoeba.org/en/">tatoeba</a>.
      From tatoeba I have downloaded the entire selection of Japanese and English, which consist of 200,000+ and 1,500,000+ sentences respectively.
      The Japanese sentences are shown in the traditional script, so to convert to hepburn I used a python package called <a href="https://github.com/polm/cutlet">cutlet</a>.
      One key difference between Japanese and English is the lack of spaces in Japanese. 
      In English we can easily separate words by spaces, but in Japanese we need an additional tokenizer to separate the words. 
      Cutlet uses MeCab and Unidict for tokenization of Japanese words.
    </p>

    <h3>Data Cleaning</h3>
    <p>
      Once both languages are in roman characters we need to further clean the data to be easier to process.
      These steps include:
      <ul>
        <li>Changing all characters to lowercase</li>
        <li>Removing all punctuation from the sentences</li>
        <li>Reduce dataset to 100,000 sentences for each</li>
      </ul>
    </p>
    <h3>Input Features</h3>
    <p>
      The classification is done solely by the frequency of letters in each sentence. 
      The letter density was calculated for each letter, this is \(\frac{\text{Number of specific letter}}{\text{Number of total letters}}\).  
      This results in 26 input features.
    </p>
    <h2 id="Preprocessing">Preprocessing</h2>
    <p>
      The data first separated into a training set and a test set, with a 99:1 ratio. This leaves 2,000 sentences for testing after the model is built.

      The data is processed using <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis</a>. This is a feature extraction technique that allows the data to be transformed into its principal components. 
      PCA works by calculating the covariance matrix for all of the input features, and calculating its eigenvectors.
      The eigenvectors are the principal components, of the directions with the largest variance in feature space.
      These directions are not specific features, but are a linear combination of all the features. 
      The dataset is then transformed into this new basis system.
      For this project, I selected the first 2 principal components which are easily visable in 2 dimensions. 
    </p>
    <img class="plot" src="images/pca1_matrix_weights.png"></img>
    <br>
    <img class="plot" src="images/pca2_matrix_weights.png"></img>
    <p>
      The images above represent the magnitude of the transformation weights in the first and second principal axes.
      These correlate to the amount of variance explainable by each letter in the principal axis. 
      This is different than the classifying power of each letter because there correlations between each letter.
      We can then normalize these graphs by the principal axis's explainable variance.
      This shows us the amount of total variance explainable by each letter in the principal axis.
    </p>
    <img class="plot" src="images/matrix_weights.png"></img>

    <h2 id="Model">Model</h2>
    <p>
      Using the first 2 principal components, a logistic regression model was trained. 
      Below we plot the 198,000 training sentences in the training set. 
      The shaded regions in the background represent the classification regions for each language. 
      There is 97.5% accuracy for the training data.  
    </p>
    <img class="plot" src="images/decision_logreg.png"></img>
    <br>
    <p>
      Lets run the test data and see how it performs. The test data also performs well with an accuracy of 97.1%.
      There is a difference of 0.38% between the training and test data. 
      When comparing to the standard error of the classification prediction we can see that it is within 1 standard deviation. 
      \[ err = \frac{\sqrt{57}}{2000} = 0.38\% \]
      Where 57 is the number of misclassified sentences and 2000 is the total sentences tested.
    </p>
    <img class="plot" src="images/decision_logreg_test.png"></img>


  </div>
</body>

</html>
